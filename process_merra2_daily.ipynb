{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERRA-2 Daily Processing Pipeline\n",
    "\n",
    "This notebook processes MERRA-2 hourly data day-by-day for the US Lower 48 states.\n",
    "\n",
    "## What this does:\n",
    "1. Loops through dates from 1984-01-01 to 2025-12-31\n",
    "2. For each day:\n",
    "   - Checks if file already exists (skips if yes)\n",
    "   - Downloads MERRA-2 M2T1NXSLV data\n",
    "   - Extracts US Lower 48 region\n",
    "   - Calculates 2m temperature (°C) and VPD (kPa)\n",
    "   - Saves to `daily_data/merra2_us_YYYYMMDD.nc`\n",
    "\n",
    "## Output:\n",
    "- **Location**: `research/daily_data/`\n",
    "- **Format**: NetCDF files named `merra2_us_YYYYMMDD.nc`\n",
    "- **Variables**: T2M (°C), VPD (kPa)\n",
    "- **Temporal**: Hourly data (24 timesteps per day)\n",
    "- **Spatial**: US Lower 48 (~0.625° x 0.5° resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our custom processing functions\n",
    "import merra2_processing as m2p\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authenticate with NASA EarthData\n",
    "\n",
    "You'll need a NASA EarthData account: https://urs.earthdata.nasa.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate once at the beginning\n",
    "auth = earthaccess.login()\n",
    "print(\"✓ Authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Lower 48 Bounding Box\n",
    "bbox = (-125, 24, -66, 49)  # (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Date range\n",
    "start_date = \"1984-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "# MERRA-2 collection\n",
    "collection_id = \"M2T1NXSLV\"  # Hourly single-level diagnostics\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"daily_data\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Date range: {start_date} to {end_date}\")\n",
    "print(f\"  Bounding box: {bbox} (US Lower 48)\")\n",
    "print(f\"  Collection: {collection_id}\")\n",
    "print(f\"  Output directory: {output_dir}\")\n",
    "\n",
    "# Generate list of dates to process\n",
    "dates = m2p.get_date_range(start_date, end_date)\n",
    "print(f\"\\nTotal days to process: {len(dates):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Existing Files\n",
    "\n",
    "Let's see how many files have already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count existing files\n",
    "existing_files = [d for d in dates if m2p.check_file_exists(d, output_dir)]\n",
    "remaining_files = [d for d in dates if not m2p.check_file_exists(d, output_dir)]\n",
    "\n",
    "print(f\"Status:\")\n",
    "print(f\"  ✓ Already processed: {len(existing_files):,} days\")\n",
    "print(f\"  ⧗ Remaining to process: {len(remaining_files):,} days\")\n",
    "\n",
    "if len(existing_files) > 0:\n",
    "    print(f\"\\nFirst processed file: {existing_files[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Last processed file: {existing_files[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process All Days\n",
    "\n",
    "This cell will loop through all dates and process them. It will:\n",
    "- Skip files that already exist\n",
    "- Show a progress bar\n",
    "- Print status for each file\n",
    "- Track success/failure/skip counts\n",
    "\n",
    "**Note**: This will take a long time for 40+ years of data. Consider processing in chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results\n",
    "results = {\n",
    "    'success': [],\n",
    "    'failed': [],\n",
    "    'skipped': []\n",
    "}\n",
    "\n",
    "# Process each day with progress bar\n",
    "for date in tqdm(dates, desc=\"Processing MERRA-2 data\"):\n",
    "    result = m2p.process_single_day(\n",
    "        date=date,\n",
    "        bbox=bbox,\n",
    "        collection_id=collection_id,\n",
    "        output_dir=output_dir,\n",
    "        auth=auth\n",
    "    )\n",
    "    \n",
    "    if result['success'] and result.get('skipped', False):\n",
    "        results['skipped'].append(date)\n",
    "    elif result['success']:\n",
    "        results['success'].append(date)\n",
    "        print(f\"✓ {date.strftime('%Y-%m-%d')}: {result['message']}\")\n",
    "    else:\n",
    "        results['failed'].append(date)\n",
    "        print(f\"✗ {date.strftime('%Y-%m-%d')}: {result['message']}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Successfully processed: {len(results['success']):,} days\")\n",
    "print(f\"Skipped (already exist): {len(results['skipped']):,} days\")\n",
    "print(f\"Failed: {len(results['failed']):,} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process a Smaller Date Range (Optional)\n",
    "\n",
    "Use this cell to process a smaller date range for testing or incremental processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process just June 2023\n",
    "test_start = \"2023-06-01\"\n",
    "test_end = \"2023-06-30\"\n",
    "\n",
    "test_dates = m2p.get_date_range(test_start, test_end)\n",
    "print(f\"Processing {len(test_dates)} days from {test_start} to {test_end}\\n\")\n",
    "\n",
    "for date in tqdm(test_dates, desc=\"Processing test range\"):\n",
    "    result = m2p.process_single_day(\n",
    "        date=date,\n",
    "        bbox=bbox,\n",
    "        collection_id=collection_id,\n",
    "        output_dir=output_dir,\n",
    "        auth=auth\n",
    "    )\n",
    "    \n",
    "    status = \"✓\" if result['success'] else \"✗\"\n",
    "    print(f\"{status} {date.strftime('%Y-%m-%d')}: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Output Files\n",
    "\n",
    "Check the structure and content of a processed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Load a sample file\n",
    "sample_files = sorted(output_dir.glob(\"merra2_us_*.nc\"))\n",
    "\n",
    "if len(sample_files) > 0:\n",
    "    sample_file = sample_files[0]\n",
    "    print(f\"Sample file: {sample_file.name}\\n\")\n",
    "    \n",
    "    ds = xr.open_dataset(sample_file)\n",
    "    \n",
    "    print(\"Dataset Information:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "    print(f\"\\nVariables: {list(ds.data_vars)}\")\n",
    "    \n",
    "    print(\"\\nT2M (Temperature):\")\n",
    "    print(f\"  Units: {ds['T2M'].attrs.get('units', 'N/A')}\")\n",
    "    print(f\"  Long name: {ds['T2M'].attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"  Range: {float(ds['T2M'].min()):.2f} to {float(ds['T2M'].max()):.2f} °C\")\n",
    "    \n",
    "    print(\"\\nVPD (Vapor Pressure Deficit):\")\n",
    "    print(f\"  Units: {ds['VPD'].attrs.get('units', 'N/A')}\")\n",
    "    print(f\"  Long name: {ds['VPD'].attrs.get('long_name', 'N/A')}\")\n",
    "    print(f\"  Range: {float(ds['VPD'].min()):.2f} to {float(ds['VPD'].max()):.2f} kPa\")\n",
    "    \n",
    "    print(\"\\nGlobal Attributes:\")\n",
    "    for key, value in ds.attrs.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    ds.close()\n",
    "else:\n",
    "    print(\"No processed files found yet. Run processing cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. List Failed Dates (if any)\n",
    "\n",
    "If any dates failed to process, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only work if you ran the processing loop above\n",
    "if 'results' in locals() and len(results['failed']) > 0:\n",
    "    print(f\"Failed dates ({len(results['failed'])}):\")\n",
    "    for date in results['failed']:\n",
    "        print(f\"  - {date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Save failed dates to a file for reprocessing\n",
    "    failed_dates_file = output_dir / \"failed_dates.txt\"\n",
    "    with open(failed_dates_file, 'w') as f:\n",
    "        for date in results['failed']:\n",
    "            f.write(f\"{date.strftime('%Y-%m-%d')}\\n\")\n",
    "    print(f\"\\nFailed dates saved to: {failed_dates_file}\")\n",
    "else:\n",
    "    print(\"No failed dates found or processing hasn't been run yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Storage Usage\n",
    "\n",
    "Check how much disk space the processed files are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total storage used\n",
    "total_size = 0\n",
    "file_count = 0\n",
    "\n",
    "for file in output_dir.glob(\"merra2_us_*.nc\"):\n",
    "    total_size += file.stat().st_size\n",
    "    file_count += 1\n",
    "\n",
    "print(f\"Storage Statistics:\")\n",
    "print(f\"  Total files: {file_count:,}\")\n",
    "print(f\"  Total size: {total_size / (1024**3):.2f} GB\")\n",
    "if file_count > 0:\n",
    "    print(f\"  Average file size: {total_size / file_count / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Processing Time Estimates:\n",
    "- Each day takes ~10-30 seconds to download and process\n",
    "- Full dataset (1984-2025): ~15,000 days = ~40-125 hours\n",
    "- **Recommendation**: Process in yearly chunks or run overnight\n",
    "\n",
    "### Tips:\n",
    "1. **Resume capability**: The code automatically skips already-processed files, so you can stop and restart anytime\n",
    "2. **Chunk processing**: Use the optional cell (Section 6) to process specific date ranges\n",
    "3. **Monitor progress**: Check the `daily_data/` directory to see files being created\n",
    "4. **Disk space**: Expect ~5-10 MB per day = ~75-150 GB for full dataset\n",
    "\n",
    "### Troubleshooting:\n",
    "- If authentication expires, re-run the authentication cell\n",
    "- If downloads are slow, check your internet connection\n",
    "- If a date consistently fails, it may not have available data (check MERRA-2 availability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
